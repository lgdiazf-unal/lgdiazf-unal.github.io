---
title: AIRFLOW
date: 2025-11-23 12:00:00 -5000
categories: [ANALITICA, ETL]
tags: [airflow,python,etl]
---

Apache Airflow es una plataforma de código abierto que permite **gestionar y automatizar** flujos de trabajo de forma declarativa. Su principal atractivo radica en la definición de DAGs (grafos acíclicos dirigidos) que describen cómo deben ejecutarse las tareas, en qué orden y con qué dependencias. Airflow provee una **planificación programada** y una **monitorización centralizada** que facilitan la supervisión y el mantenimiento de pipelines tanto en arquitecturas de datos como en proyectos de machine learning.

## Instalacion


Para desplegar Airflow de manera rápida y confiable, utilizaremos la [guía oficial con Docker Compose](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#fetching-docker-compose-yaml). La descarga y puesta en marcha siguen los pasos siguientes:


**1. Descargar el fichero `docker-compose.yml` desde la documentación oficial**

```bash
curl -LfO 'https://airflow.apache.org/docs/apache-airflow/3.1.3/docker-compose.yaml'
```
**2. Iniciar la stack de Docker**

```bash
docker compose up -d
```

**Nota :** El despliegue inicial puede tardar varios minutos porque Docker necesita descargar las imágenes correspondientes a cada contenedor. Una vez finalizado, los servicios estarán accesibles y listos para usarse.


## Descripción del stack

El archivo **docker‑compose.yml** define **ocho** servicios que, en conjunto, forman una instalación completa y autónoma de Apache Airflow. Cada uno cumple una función específica dentro de la orquestación y ejecución de los flujos de trabajo.

| Servicio                 | Propósito |
|--------------------------|-----------|
| **airflow-scheduler**    | Supervisar todos los DAGs y tareas, y disparar las instancias de tarea cuando se hayan cumplido sus dependencias. |
| **airflow-dag-processor** | Analizar y validar los archivos DAG antes de que entren en la cola del scheduler. |
| **airflow-api‑server**   | Exponer la API REST y la UI en `http://localhost:8080`. |
| **airflow-worker**       | Ejecutar las tareas asignadas por el scheduler. |
| **airflow-triggerer**    | Ejecutar un bucle de evento para tareas diferibles (deferrable). |
| **airflow-init**         | Inicializar Airflow (configurar la BD, crear usuarios por defecto, etc.). |
| **postgres**             | Almacén persistente de la base de datos relacional que almacena configuración y metadatos. |
| **redis**                | Broker de mensajes que dirige las tareas del scheduler a los workers. |

## Mapeo de Directorios

Al arrancar la solución se montan varias carpetas del host dentro del contenedor, lo que permite que su contenido se sincronice de forma bidireccional.  
En otras palabras, cualquier fichero que coloques en estas carpetas en tu máquina será accesible dentro del ambiente de Airflow, y los ficheros generados por el contenedor (por ejemplo logs o configuraciones) aparecerán en tu disco local.

| Carpeta en el host | Descripción |
|--------------------|-------------|
| `./dags` | Coloca aquí los archivos de tus DAGs. Airflow los inspeccionará y los cargará automáticamente. |
| `./logs` | Contiene los logs generados durante la ejecución de tareas y por el scheduler. Útil para depuración. |
| `./config` | Puedes añadir tu propio *log parser* o el fichero `airflow_local_settings.py` para configurar políticas de cluster, entre otras personalizaciones. |
| `./plugins` | Espacio destinado a plugins de Airflow personalizados (operator, hook, etc.). |
